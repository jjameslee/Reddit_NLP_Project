{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap:\n",
    "In the previous notebook, we tested and evaluated the Logistic Regression model. We originally tested a naive Logistic Regression model without inputting much parameters and unsurprisingly got a not-so-great score on our accuracy. Afterwards we created a pipeline that will consist to two stages:\n",
    "* An instance of CountVectorizer\n",
    "* A LogisticRegression instance\n",
    "\n",
    "Through GridSearching for the optimal set of hyperparameters for our CountVectorizer, we got a Train accuracy score of 0.886 and\n",
    "Test accuracy score of 0.765. A big improvement over the naive Logistic Regression model score. \n",
    "Through this model, we got the top 20 most important features:\n",
    "* blinks, yg, vip, dddd, square, cf, ga, wig, whistle, scandal, queue, axe, d4, chanel, revolution, teddy, area, queens, diaries, aiiy\n",
    "\n",
    "Confusion Matrix (shows how our model performed):\n",
    "* True Negatives: 1795\n",
    "* False Positives: 585\n",
    "* False Negatives: 515\n",
    "* True Positives: 1789\n",
    " \n",
    "\n",
    "### Next Steps: \n",
    "\n",
    "In this notebook, we will begin by identifying what Naive Bayes model we should use (Bernoulli, Multinomial, Gaussian). We will split the data for validation and training purposes. Ultimately testing and evaluating a Naive Bayes modeling technique to hopefully identify a production algorithm. We will compare this model's score to not only the baseline accuracy score but also the Logistic Regression's score. Throughout this notebook, we will explain the process of the model and evaluate the outcome of the Naive Bayes model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_Push_Shift_API.ipynb      06_Random_Forest_Model.ipynb\n",
      "02_Cleaning_EDA.ipynb        \u001b[34mUntitled Folder\u001b[m\u001b[m\n",
      "03_Preprocessing.ipynb       \u001b[34massets\u001b[m\u001b[m\n",
      "04_LogReg_Model.ipynb        \u001b[34mdata\u001b[m\u001b[m\n",
      "05_Naive_Bayes_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>blackpink</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is something i can get behind and appreci...</td>\n",
       "      <td>1</td>\n",
       "      <td>1043</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hold the fuck up   rock songs  i m considering...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what time and date is this in pdt  0 am is a b...</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is there a list  last year i remember they str...</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as a blink i ll wait till mv dropped  then i l...</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  blackpink  char_count  \\\n",
       "0  this is something i can get behind and appreci...          1        1043   \n",
       "1  hold the fuck up   rock songs  i m considering...          1          75   \n",
       "2  what time and date is this in pdt  0 am is a b...          1         106   \n",
       "3  is there a list  last year i remember they str...          1          84   \n",
       "4  as a blink i ll wait till mv dropped  then i l...          1         149   \n",
       "\n",
       "   word_count  \n",
       "0         188  \n",
       "1          12  \n",
       "2          27  \n",
       "3          16  \n",
       "4          33  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/bp_bts_df_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy:\n",
    "\n",
    "Our baseline model accuracy is still 51%. Once again, this means we will be correct 51% of the time if we choose that a comment is from the majority class subreddit, which in this case is the 'bangtan' subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.blackpink.value_counts(normalize=True).max(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous notebook, we will implement the CountVectorizer tool. However, this time we only really care about the maximum number of counts of the 6,500 max features it pulled from the text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['body']\n",
    "y = df['blackpink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "# stratify = y --> keeps the proportion of the class the same in the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assets/stopwords.pkl','rb') as f:\n",
    "    stopwords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>09</th>\n",
       "      <th>0am</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>yymmdd</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zedd</th>\n",
       "      <th>zelle</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.000000</td>\n",
       "      <td>14052.00000</td>\n",
       "      <td>14052.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.032656</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>0.020660</td>\n",
       "      <td>0.068518</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>0.020660</td>\n",
       "      <td>0.020662</td>\n",
       "      <td>0.025304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039538</td>\n",
       "      <td>0.025304</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.027975</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>0.04044</td>\n",
       "      <td>0.023857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 6500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 00           000            01            02            04  \\\n",
       "count  14052.000000  14052.000000  14052.000000  14052.000000  14052.000000   \n",
       "mean       0.001566      0.001067      0.000427      0.000427      0.000427   \n",
       "std        0.049166      0.032656      0.023857      0.023857      0.020660   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        2.000000      1.000000      2.000000      2.000000      1.000000   \n",
       "\n",
       "                 05            06            07            09           0am  \\\n",
       "count  14052.000000  14052.000000  14052.000000  14052.000000  14052.000000   \n",
       "mean       0.001566      0.000427      0.000427      0.000285      0.000498   \n",
       "std        0.068518      0.026674      0.020660      0.020662      0.025304   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        5.000000      2.000000      1.000000      2.000000      2.000000   \n",
       "\n",
       "       ...           yup        yymmdd       zealand          zedd  \\\n",
       "count  ...  14052.000000  14052.000000  14052.000000  14052.000000   \n",
       "mean   ...      0.001566      0.000498      0.000285      0.000285   \n",
       "std    ...      0.039538      0.025304      0.016870      0.016870   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.000000      2.000000      1.000000      1.000000   \n",
       "\n",
       "              zelle          zero          zeus        zodiac         zone  \\\n",
       "count  14052.000000  14052.000000  14052.000000  14052.000000  14052.00000   \n",
       "mean       0.000285      0.000712      0.000498      0.000213      0.00121   \n",
       "std        0.016870      0.026668      0.027975      0.018863      0.04044   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "max        1.000000      1.000000      2.000000      2.000000      2.00000   \n",
       "\n",
       "              zones  \n",
       "count  14052.000000  \n",
       "mean       0.000427  \n",
       "std        0.023857  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        2.000000  \n",
       "\n",
       "[8 rows x 6500 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our CountVectorizer.\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                      stop_words = stopwords,\n",
    "                      max_features = 6500,\n",
    "                      min_df=2,\n",
    "                      max_df=0.98)\n",
    "\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "\n",
    "train_cvec_df = pd.DataFrame(X_train_cvec.todense(),   # b/c it is saved as a df...\n",
    "                                    columns = cvec.get_feature_names())\n",
    "test_cvec_df = pd.DataFrame(X_test_cvec.todense(), \n",
    "                            columns = cvec.get_feature_names())\n",
    "\n",
    "train_cvec_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Naive Bayes\n",
    "\n",
    "When deciding which of the three Naive Bayes Model to use, it is good practice to see a summary statistic of the dataframe after converting a collection of text documents into a matrix vector of token counts. We can use the max counts of each words in the dataframe to help determine the model. In column '05', we can see the maximum count is 5, and for most of the columns the max range from either 1 or 2. \n",
    "\n",
    "Picking the best one of the 3 Naive Bayes model choices:\n",
    "    \n",
    "- BernoulliNB is best when we have 0/1 counts in all columns of X. (a.k.a. dummy variables)\n",
    "- GaussianNB is best when the columns of X are Normally distributed. (Or whenever BernoulliNB and MultinomialNB are inappropriate.)\n",
    "- **The columns of X are all integer counts, so MultinomialNB is the best choice here.**\n",
    "\n",
    "The Naive Bayes assumes that the independant variables (features) are independant of one another in order to reduce the complexity of conditional probabilities (Baye's Theorem). Though this assumption is especially flawed when it comes to text data, this model still produces a surprisingly accurate prediction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our Multinomial Naive Bayes' model!\n",
    "nb =  MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our model!\n",
    "nb.fit(train_cvec_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our predictions!\n",
    "pred = nb.predict(test_cvec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8179618559635639"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score our model on the training set.\n",
    "\n",
    "nb.score(train_cvec_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764730999146029"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_cvec_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes Model is commonly used with the Tfidf feature-extraction tool. The TFIDF score tells us which words are most discriminating. In other words, if the number of times a word appears in a comment is frequent while the number of times it appears in the overall collection of comments is rare, then the TFIDF score increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='word', stop_words=stopwords)),\n",
    "    ('nb', MultinomialNB())\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581838884144606\n",
      "{'tfidf__max_df': 0.95, 'tfidf__max_features': 6500, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1), 'tfidf__strip_accents': 'unicode'}\n",
      "Train accuracy score: 0.8437944776544264\n",
      "Test accuracy score: 0.767933390264731\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'tfidf__strip_accents': ['unicode', None],\n",
    "    'tfidf__ngram_range' : [(1,1)],\n",
    "    'tfidf__max_df' : [.95, .98, 1.0],\n",
    "    'tfidf__min_df' : [1, 2, 5],\n",
    "    'tfidf__max_features': [3000, 5000, 6500]  #does no stop_words w/ 1000 then 1500; THEN english stop_words w/ 1000 then 1500...\n",
    "\n",
    "}\n",
    "gs = GridSearchCV(pipe, param_grid=params, cv=3)   #cv do 2-3 for project; save time.\n",
    "gs.fit(X_train, y_train) # also does cv in the background\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Train score\n",
    "print(f'Train accuracy score: {gs.score(X_train, y_train)}')\n",
    "\n",
    "# Test score\n",
    "print(f'Test accuracy score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the grid search for our Multinomial N.B model, it selected 6,500 features to be the best max features, chose 1 grams as the best ngram_range, 1 min as the best minimum words, and .95 for the max df as the best maximum threshold. Though our training score increased a bit, the testing score did not have any noticeable changes. Therefore, it seems uncessary to invest time/money into GridSearching across different sets of hyperparameters with not much of a positive outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer = \"word\",\n",
    "                        strip_accents = 'unicode',\n",
    "                      stop_words = stopwords,\n",
    "                      max_features = 6500,\n",
    "                      max_df=0.95,\n",
    "                       min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "train_tfidf_df = pd.DataFrame(X_train_tfidf.todense(),   # b/c it is saved as a df...\n",
    "                                    columns = tfidf.get_feature_names())\n",
    "test_tfidf_df = pd.DataFrame(X_test_tfidf.todense(), \n",
    "                            columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>09</th>\n",
       "      <th>0am</th>\n",
       "      <th>...</th>\n",
       "      <th>yymmdd</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zedd</th>\n",
       "      <th>zelle</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01   02   04   05   06   07   09  0am  ...  yymmdd  zealand  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0      0.0   \n",
       "\n",
       "   zedd  zelle  zero  zeus  zhou  zodiac  zone  zones  \n",
       "0   0.0    0.0   0.0   0.0   0.0     0.0   0.0    0.0  \n",
       "1   0.0    0.0   0.0   0.0   0.0     0.0   0.0    0.0  \n",
       "2   0.0    0.0   0.0   0.0   0.0     0.0   0.0    0.0  \n",
       "3   0.0    0.0   0.0   0.0   0.0     0.0   0.0    0.0  \n",
       "4   0.0    0.0   0.0   0.0   0.0     0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 6500 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(train_tfidf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8437944776544264"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(train_tfidf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.767933390264731"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_tfidf_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "We will implement the confusion matrix in order to help us visualize how our model performs. \n",
    "We will use the fitted model to predict on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nb.predict(test_tfidf_df)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1928,  452],\n",
       "       [ 635, 1669]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 1928\n",
      "False Positives: 452\n",
      "False Negatives: 635\n",
      "True Positives: 1669\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a **true positive** mean here?:\n",
    "\n",
    "- True positives are comments we correctly predict to be positive.\n",
    "- In this case, since Blackpink = 1 (Bts=0), a true positive means the model correctly predicted 1,669 comments to be from the Blackpink subreddit.\n",
    "\n",
    "---\n",
    "\n",
    "What does a **true negative** mean here?:\n",
    "\n",
    "- True negatives are comments we correctly predict to be negative.\n",
    "- In this case, since Blackpink = 1 (Bts = 0), a true negative means the model correctly predicted 1,928 comments to be from the Bts subreddit.\n",
    "\n",
    "---\n",
    "\n",
    "What does a **false positive** mean here?:\n",
    "\n",
    "- False positives are comments we falsely predict to be positive.\n",
    "- In this case, since Blackpink = 1 (Bts = 0), a false positive means the model incorrectly predicted 452 comments to be from the Blackpink subreddit (when it's really from the Bts subreddit).\n",
    "\n",
    "---\n",
    "\n",
    "What does a **false negative** mean here?:\n",
    "\n",
    "- False negatives are comments we false predict to be negative.\n",
    "- In this case, since Blackpink = 1 (Bts=0), a false negative means the model incorrectly predicted 635 comments to be from the Bts subreddit (when it's really from the Blackpink subreddit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Please continue to Notebook 06: Random Forest Model**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
